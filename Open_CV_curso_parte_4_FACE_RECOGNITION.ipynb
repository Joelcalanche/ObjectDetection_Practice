{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "black-wayne",
   "metadata": {},
   "source": [
    "# Train  Face Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "clean-satin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ben Afflek', 'Elton John', 'Jerry Seinfield', 'Madonna', 'Mindy-Kaling']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "people = ['Ben Afflek', 'Elton John', 'Jerry Seinfield', 'Madonna', 'Mindy-Kaling']\n",
    "p = []\n",
    "\n",
    "for i in os.listdir(r'C:\\Users\\calanche\\Desktop\\openCv_practice\\opencv-course\\Resources\\Faces\\train'):\n",
    "    p.append(i)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "another-attraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = r'C:\\Users\\calanche\\Desktop\\openCv_practice\\opencv-course\\Resources\\Faces\\train'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "commercial-stevens",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = p.index('Ben Afflek')\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-detroit",
   "metadata": {},
   "source": [
    "The idea behind converting a label to numerical values is essentially reducing the straint that your computer will have, by creating some sort of mapping between a string and the numerical label. Now the mapping of we are going to do is essentially the index of that particular list. \n",
    "\n",
    "en este caso por ejemplo 'Ben Afflek' corresponde al index 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "indonesian-recruitment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\calanche\\Desktop\\openCv_practice\\opencv-course\\Resources\\Faces\\train\\Ben Afflek\n",
      "['1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '2.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "C:\\Users\\calanche\\Desktop\\openCv_practice\\opencv-course\\Resources\\Faces\\train\\Elton John\n",
      "['1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '2.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "C:\\Users\\calanche\\Desktop\\openCv_practice\\opencv-course\\Resources\\Faces\\train\\Jerry Seinfield\n",
      "['1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "C:\\Users\\calanche\\Desktop\\openCv_practice\\opencv-course\\Resources\\Faces\\train\\Madonna\n",
      "['1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "C:\\Users\\calanche\\Desktop\\openCv_practice\\opencv-course\\Resources\\Faces\\train\\Mindy-Kaling\n",
      "['1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n"
     ]
    }
   ],
   "source": [
    " for person in p:\n",
    "        path = os.path.join(DIR, person)\n",
    "        print(path)\n",
    "        print(os.listdir(path))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "welcome-segment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training done\n",
      "length of the features = 100\n",
      "length of the labels = 100\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "labels = []\n",
    "# nota \"roi\" = region of interest\n",
    "face_cascade = cv.CascadeClassifier('./image_samples/haarcascade_frontalface_default.xml')\n",
    "def create_train():\n",
    "    for person in p:\n",
    "        path = os.path.join(DIR, person)\n",
    "#         print(path)\n",
    "        # index of list of classes\n",
    "        \n",
    "        label = p.index(person)\n",
    "        \n",
    "        for img in os.listdir(path):\n",
    "            img_path = os.path.join(path,img)\n",
    "#             print(img_path)\n",
    "            img_array = cv.imread(img_path)\n",
    "            gray = cv.cvtColor(img_array, cv.COLOR_BGR2GRAY)\n",
    "            \n",
    "            faces_rect = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)\n",
    "            \n",
    "            for (x,y,w,h) in faces_rect:\n",
    "                # filas(y) columnnas(x)\n",
    "                faces_roi = gray[y:y+h, x:x+w]\n",
    "                \n",
    "                features.append(faces_roi)\n",
    "                labels.append(label)\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "create_train()\n",
    "print('training done')\n",
    "\n",
    "print(f\"length of the features = {len(features)}\")\n",
    "print(f\"length of the labels = {len(labels)}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "retired-intersection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "features = np.array(features, dtype='object')\n",
    "\n",
    "labels = np. array(labels)\n",
    "\n",
    "\n",
    "# instantiate face recognizer\n",
    "face_recognizer = cv.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "\n",
    "# Train the Recognizer on features list and labels list\n",
    "face_recognizer.train(features, labels)\n",
    "\n",
    "# save the model\n",
    "face_recognizer.save('face_trained.yml')\n",
    "\n",
    "np.save('features.npy', features,  allow_pickle=True)\n",
    "\n",
    "np.save('labels.npy', labels, allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-basement",
   "metadata": {},
   "source": [
    "note:  if we plan to use this face recognizer in another  file, we'll have to separately and manually repeat this process, this whole process of adding those images to a list and getting the corresponding labels, and then coverting that to NumPy array, and then training all over again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-referral",
   "metadata": {},
   "source": [
    "## Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "yellow-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread(r'C:\\Users\\calanche\\Desktop\\openCv_practice\\opencv-course\\Resources\\Faces\\val\\ben_afflek\\2.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "nuclear-pierre",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label= Madonna with a confidence of 78.75449489280508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import cv2 as cv\n",
    "\n",
    "haar_cascade = cv.CascadeClassifier('./image_samples/haarcascade_frontalface_default.xml')\n",
    "people = ['Ben Afflek', 'Elton John', 'Jerry Seinfield', 'Madonna', 'Mindy-Kaling']\n",
    "# features = np.load('features.npy', allow_pickle=True)\n",
    "\n",
    "# labels = np.load('labels.npy', allow_pickle=True)\n",
    "# instantiate a model\n",
    "face_recognizer = cv.face.LBPHFaceRecognizer_create()\n",
    "# load the patterns learned\n",
    "\n",
    "face_recognizer.read('face_trained.yml')\n",
    "\n",
    "\n",
    "# img = cv.imread(r'C:\\Users\\calanche\\Desktop\\openCv_practice\\opencv-course\\Resources\\Faces\\val\\ben_afflek\\5.jpg')\n",
    "img = cv.imread(r'C:\\Users\\calanche\\Desktop\\openCv_practice\\opencv-course\\Resources\\Faces\\val\\madonna\\3.jpg')\n",
    "\n",
    "gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "cv.imshow('Person', gray)\n",
    "\n",
    "# Detect the face in the image\n",
    "\n",
    "faces_rect = haar_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "for (x,y,w,h) in faces_rect:\n",
    "    faces_roi = gray[y:y+h, x:x+w]\n",
    "    \n",
    "    label, confidence = face_recognizer.predict(faces_roi)\n",
    "    \n",
    "    print(f'label= {people[label]} with a confidence of {confidence}')\n",
    "    \n",
    "    cv.putText(img, str(people[label]), (20, 20), cv.FONT_HERSHEY_COMPLEX, 1.0, (0, 255, 0), thickness=2)\n",
    "    \n",
    "    cv.rectangle(img, (x,y), (x+w, y+h), (0, 255, 0), thickness=2)\n",
    "    \n",
    "    \n",
    "cv.imshow('Detected Face', img)\n",
    "\n",
    "cv.waitKey(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-courtesy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
